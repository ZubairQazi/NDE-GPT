{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/openai/openai-cookbook/blob/main/examples/Chat_finetuning_data_prep.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a06ec76c",
   "metadata": {},
   "source": [
    "# Data preparation and analysis for chat model fine-tuning\n",
    "\n",
    "This notebook serves as a tool to preprocess and analyze the chat dataset used for fine-tuning a chat model. \n",
    "It checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs.\n",
    "The method shown here corresponds to the [current fine-tuning method](https://platform.openai.com/docs/guides/fine-tuning) for gpt-3.5-turbo.\n",
    "See [legacy fine-tuning](https://platform.openai.com/docs/guides/legacy-fine-tuning) for models like babbage-002 and davinci-002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e63973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "013bdbc4",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "We first load the chat dataset from an [example JSONL file](https://github.com/openai/openai-cookbook/blob/main/examples/data/toy_chat_fine_tuning.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c248ccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 100\n",
      "First example:\n",
      "{'role': 'system', 'content': 'Generate a comma-separated list of relevant EDAM topics based on the provided abstract and topic categories.'}\n",
      "{'role': 'user', 'content': 'An abstract associated with a scientific dataset is quoted below:\\n\\n\"Mobile genetic elements threaten genome integrity in all organisms. RDE-3 (also known as MUT-2) is a ribonucleotidyltransferase that is required for transposon silencing and RNA interference in Caenorhabditis elegans<sup>1-4</sup>. When tethered to RNAs in heterologous expression systems, RDE-3 can add long stretches of alternating non-templated uridine (U) and guanosine (G) ribonucleotides to the 3\\' termini of these RNAs (designated poly(UG) or pUG tails)<sup>5</sup>. Here we show that, in its natural context in C. elegans, RDE-3 adds pUG tails to targets of RNA interference, as well as to transposon RNAs. RNA fragments attached to pUG tails with more than 16 perfectly alternating 3\\' U and G nucleotides become gene-silencing agents. pUG tails promote gene silencing by recruiting RNA-dependent RNA polymerases, which use pUG-tailed RNAs (pUG RNAs) as templates to synthesize small interfering RNAs (siRNAs). Our results show that cycles of pUG RNA-templated siRNA synthesis and siRNA-directed pUG RNA biogenesis underlie double-stranded-RNA-directed transgenerational epigenetic inheritance in the C. elegans germline. We speculate that this pUG RNA-siRNA silencing loop enables parents to inoculate progeny against the expression of unwanted or parasitic genetic elements.\"\\n\\nSelect the 8 most-relevant topic categories from the list. Provide your answers as a comma-separated list. Ensure your answers exactly match one of the provided categories below. Do not include categories that are not listed. \\n\\nAcoustics\\nAgricultural science\\n\"Allergy, clinical immunology and immunotherapeutics\"\\nAnaesthesiology\\nAnalytical chemistry\\nAnatomy\\nAnimal study\\nAntimicrobial Resistance\\nBiobank\\nBiochemistry\\nBiodiversity\\nBioengineering\\nBioinformatics\\nBiology\\nBiomarkers\\nBiomaterials\\nBiomedical science\\nBiomolecular simulation\\nBiophysics\\nBiosciences\\nBiotechnology\\nBiotherapeutics\\nCarbohydrates\\nCarbon cycle\\nCardiology\\nCell biology\\nCell culture collection\\nChemical biology\\nCheminformatics\\nChemistry\\nChemometrics\\nCladistics\\nClone library\\nComparative genomics\\nComplementary medicine\\nCompound libraries and screening\\nComputational biology\\nComputational chemistry\\nComputer science\\nCopy number variation\\nCritical care medicine\\nCryogenic electron microscopy\\nCytogenetics\\nDNA\\nDNA binding sites\\nDNA mutation\\nDNA packaging\\nDNA polymorphism\\nDNA replication and recombination\\nData acquisition\\n\"Data architecture, analysis and design\"\\nData governance\\nData identity and mapping\\nData integration and warehousing\\nData management\\nData mining\\nData quality management\\nData rescue\\nData security\\n\"Data submission, annotation, and curation\"\\nData visualisation\\nDatabase management\\nDentistry\\nDermatology\\nDevelopmental biology\\nDrug development\\nDrug discovery\\nDrug metabolism\\n\"Ear, nose and throat medicine\"\\nEcology\\nElectrocardiography\\nElectroencephalography\\nElectron microscopy\\nEmbryology\\nEndocrinology and metabolism\\nEnvironmental sciences\\nEnzymes\\nEpigenetics\\nEpigenomics\\nEpistasis\\nEvolutionary biology\\nFAIR data\\nFluxomics\\nFreshwater biology\\nFunction analysis\\nFunctional genomics\\n\"Functional, regulatory and non-coding RNA\"\\nGastroenterology\\nGender medicine\\nGene and protein families\\nGene expression\\nGene regulation\\nGene structure\\nGene transcripts\\nGenetic engineering\\nGenetic variation\\nGenetics\\nGenomic imprinting\\nGenomics\\nGenotype and phenotype\\nGeriatric medicine\\nGynaecology and obstetrics\\nHaematology\\nHepatic and biliary medicine\\nHuman biology\\nHuman genetics\\nImmunogenetics\\nImmunoinformatics\\nImmunology\\nImmunomics\\nImmunoproteins and antigens\\nInfectious disease\\nInformatics\\nLaboratory animal science\\nLaboratory information management\\nLipids\\nMachine learning\\nMapping\\nMarine biology\\nMedical biotechnology\\nMedical informatics\\nMedical toxicology\\nMedicinal chemistry\\nMedicine\\nMedicines research and development\\nMembrane and lipoproteins\\nMetabarcoding\\nMetabolic engineering\\nMetabolomics\\nMetagenomics\\nMetatranscriptomics\\nMicrobial collection\\nMicrobial ecology\\nMicrobiology\\nMicrofluidics\\nMobile genetic elements\\nModel organisms\\nMolecular biology\\nMolecular dynamics\\nMolecular evolution\\nMolecular genetics\\n\"Molecular interactions, pathways and networks\"\\nMolecular medicine\\nMolecular modelling\\nMouse clinic\\nMultiomics\\nMusculoskeletal medicine\\nNMR\\nNatural language processing\\nNeurobiology\\nNeurology\\nNeutron diffraction\\n\"Nucleic acid sites, features and motifs\"\\nNucleic acid structure analysis\\nNucleic acids\\nNutritional science\\nOmics\\nOncology\\nOntology and terminology\\nOpen science\\nOphthalmology\\nPaediatrics\\nPain medicine\\nPaleogenomics\\nParasitology\\nPathology\\nPersonalised medicine\\nPharmacogenomics\\nPharmacology\\nPharmacovigilance\\nPhenomics\\nPhylogenetics\\nPhylogenomics\\nPhylogeny\\nPhysics\\nPhysiology\\nPlant biology\\nPopulation genetics\\nPopulation genomics\\nPreclinical and clinical studies\\nProbes and primers\\nProtein binding sites\\nProtein disordered structure\\nProtein expression\\n\"Protein folding, stability and design\"\\nProtein folds and structural domains\\nProtein interactions\\nProtein modifications\\nProtein properties\\nProtein secondary structure\\n\"Protein sites, features and motifs\"\\nProtein structural motifs and surfaces\\nProtein structure analysis\\nProtein targeting and localisation\\nProtein variants\\nProteins\\nProteogenomics\\nProteomics\\nPsychiatry\\nPublic health and epidemiology\\nQuality affairs\\nQuantitative genetics\\nRNA\\nRNA splicing\\nRare diseases\\nRegenerative medicine\\nRegulatory affairs\\nReproductive health\\nRespiratory medicine\\nRibosome Profiling\\nSafety sciences\\nSample collections\\nSequence analysis\\nSequence assembly\\n\"Sequence composition, complexity and repeats\"\\n\"Sequence sites, features and motifs\"\\nSmall molecules\\nSoftware engineering\\nStructural biology\\nStructural genomics\\nStructural variation\\nStructure analysis\\nStructure prediction\\nSurgery\\nSynthetic biology\\nSynthetic chemistry\\nSystems biology\\nSystems medicine\\nTaxonomy\\nToxicology\\nTranscription factors and regulatory sites\\nTranscriptomics\\nTranslational medicine\\nTrauma medicine\\nTropical medicine\\nUrology and nephrology\\nVaccinology\\nVeterinary medicine\\nVirology\\nWorkflows\\nX-ray diffraction\\nZoology\\n\\nPlease refrain from providing explanations. Take a moment and answer to the best of your ability.\\n'}\n",
      "{'role': 'assistant', 'content': 'Animal study, Epigenetics, Genetics, Laboratory animal science, Proteins, RNA, Small molecules, Zoology'}\n"
     ]
    }
   ],
   "source": [
    "data_path = input('Enter jsonl file path:')\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17903d61",
   "metadata": {},
   "source": [
    "## Format validation\n",
    "\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. **Data Type Check**: Checks whether each entry in the dataset is a dictionary (`dict`). Error type: `data_type`.\n",
    "2. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`.\n",
    "3. **Message Keys Check**: Validates that each message in the `messages` list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
    "4. **Unrecognized Keys in Messages**: Logs if a message has keys other than `role`, `content`, and `name`. Error type: `message_unrecognized_key`.\n",
    "5. **Role Validation**: Ensures the `role` is one of \"system\", \"user\", or \"assistant\". Error type: `unrecognized_role`.\n",
    "6. **Content Validation**: Verifies that `content` has textual data and is a string. Error type: `missing_content`.\n",
    "7. **Assistant Message Presence**: Checks that each conversation has at least one message from the assistant. Error type: `example_missing_assistant_message`.\n",
    "\n",
    "The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9f3ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "981e77da",
   "metadata": {},
   "source": [
    "## Token Counting Utilities\n",
    "\n",
    "Lets define a few helpful utilities to be used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f4b47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fdff67d",
   "metadata": {},
   "source": [
    "## Data Warnings and Token Counts \n",
    "\n",
    "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
    "\n",
    "1. **Missing System/User Messages**: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
    "2. **Number of Messages Per Example**: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
    "3. **Total Tokens Per Example**: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
    "4. **Tokens in Assistant's Messages**: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
    "5. **Token Limit Warnings**: Checks if any examples exceed the maximum token limit (4096 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52e58ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 1235, 1722\n",
      "mean / median: 1469.76, 1472.0\n",
      "p5 / p95: 1353.0, 1599.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 3, 59\n",
      "mean / median: 27.86, 27.5\n",
      "p5 / p95: 15.9, 42.10000000000001\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2afb04df",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "In this final section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb95a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~146976 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~440928 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0ad0369",
   "metadata": {},
   "source": [
    "See https://openai.com/pricing to estimate total costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "\n",
    "As of Dec. 4 2023, the price for gpt-3.5-turbo-1106 is:\n",
    "\n",
    "Training - $0.0080 / 1K tokens\n",
    "\n",
    "Input - $0.0030 / 1K tokens\n",
    "\n",
    "Output - $0.0060 / 1K tokens\n",
    "\n",
    "\n",
    "```total cost = base cost per 1k tokens * number of tokens in the input file * number of epochs trained```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost: $3.53\n"
     ]
    }
   ],
   "source": [
    "training_cost = 0.008 / 1000 * n_billing_tokens_in_dataset * n_epochs\n",
    "print('Training Cost:', f'${training_cost:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not including output cost (assume + 2/3 training cost at minimum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
