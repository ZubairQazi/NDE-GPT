{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import text2term\n",
    "\n",
    "import ast\n",
    "import csv\n",
    "\n",
    "from collections import Counter, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/staging_test_set.csv').rename(columns={'EDAM Topics': 'Old EDAM Topics'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list(string):\n",
    "    try:\n",
    "        return ast.literal_eval(string)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "dataset['MeSH Terms'] = dataset['MeSH Terms'].apply(convert_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty MeSH\n",
    "dataset = dataset[~dataset['MeSH Terms'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_slash_or_comma(lst):\n",
    "    return any('/' in string or ',' in string for string in lst)\n",
    "\n",
    "def has_forward_slash_or_comma(df, column_name):\n",
    "    return df[column_name].apply(has_slash_or_comma)\n",
    "\n",
    "has_forward_slash_or_comma(dataset, 'MeSH Terms').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_strings(lst):\n",
    "    new_list = []\n",
    "    for string in lst:\n",
    "        filtered_str = string.replace('*', '')\n",
    "        if '/' in filtered_str:\n",
    "            new_list.extend([str.strip() for str in filtered_str.split('/')])\n",
    "        elif ',' in filtered_str:\n",
    "            # new_list.extend([str.strip() for str in filtered_str.split(',')])\n",
    "            new_list.extend(map(str.strip, next(csv.reader([string])), skipinitialspace=True))\n",
    "        else:\n",
    "            new_list.append(filtered_str)\n",
    "    return np.unique(new_list).tolist()\n",
    "\n",
    "dataset['Filtered MeSH Terms'] = dataset['MeSH Terms'].apply(split_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Filtered MeSH Terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_lists(df, column_name):\n",
    "    return [item for sublist in df[column_name] for item in sublist]\n",
    "\n",
    "all_mesh_terms = flatten_lists(dataset, 'Filtered MeSH Terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_term_freqs = Counter(all_mesh_terms)\n",
    "unique_mesh_terms = set(all_mesh_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frequency = min(mesh_term_freqs.values())\n",
    "strings_with_min_frequency = [string for string, frequency in mesh_term_freqs.items() if frequency == min_frequency]\n",
    "\n",
    "print('Minimum frequency:', min_frequency)\n",
    "print('Terms with min frequency:', len(strings_with_min_frequency), '/', len(unique_mesh_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text2term scores\n",
    "\n",
    "edam_ontology = text2term.cache_ontology(\"https://data.bioontology.org/ontologies/EDAM/submissions/44/download?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb\", \"EDAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_terms = text2term.map_terms(list(unique_mesh_terms), \"EDAM\", use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_terms = mapped_terms[mapped_terms['Mapped Term IRI'].str.contains('topic')]\n",
    "mapped_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_terms[mapped_terms['Mapped Term Label'] == 'Animal study']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mapped_terms['Mapping Score'].values, bins='auto', edgecolor='black', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Mapping Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.axvline(np.mean(mapped_terms['Mapping Score'].values), color='red')\n",
    "plt.axvline(np.median(mapped_terms['Mapping Score'].values), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = np.mean(mapped_terms['Mapping Score'].values)\n",
    "threshold = 0.7\n",
    "\n",
    "def map_mesh_to_edam(mesh_terms):\n",
    "    # mapping = text2term.map_terms(mesh_terms, \"EDAM\", use_cache=True)\n",
    "    try:\n",
    "         filtered_mapping = text2term.map_terms(mesh_terms, \"EDAM\", use_cache=True)\n",
    "        # filtered_mapping = text2term.map_terms([term for term in mesh_terms if mesh_term_freqs[term] > 1], \"EDAM\", use_cache=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # Only consider mappings over threshold\n",
    "    filtered_terms = filtered_mapping[filtered_mapping['Mapping Score'] > threshold]\n",
    "\n",
    "    return filtered_terms['Mapped Term Label'].unique().tolist()\n",
    "\n",
    "dataset['New EDAM Topics'] = dataset['Filtered MeSH Terms'].apply(map_mesh_to_edam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no mapped terms\n",
    "dataset = dataset[~dataset['New EDAM Topics'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any EDAM not in the list\n",
    "with open(input(\"Enter EDAM topics file:\"), 'r') as edam_file:\n",
    "    full_edam_topics = edam_file.readlines()\n",
    "\n",
    "full_edam_topics = [topic.strip() for topic in full_edam_topics]\n",
    "\n",
    "dataset['New EDAM Topics'] = dataset['New EDAM Topics'].apply(lambda x: [item for item in x if item in full_edam_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare old and new (dataset vs outputs.csv)\n",
    "\n",
    "gpt_output = pd.read_csv('outputs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_edam(abstract):\n",
    "    # topics = dataset.loc[dataset['Abstract'] == abstract, 'New EDAM Topics'].values\n",
    "    # return topics[0] if len(topics[0]) > 0 else None\n",
    "\n",
    "    matching_rows = dataset.loc[dataset['Abstract'] == abstract, 'New EDAM Topics']\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        return matching_rows.iloc[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(get_new_edam(gpt_output.iloc[0]['Abstract']))\n",
    "print(gpt_output['Ground Truth'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output.rename(columns={'Ground Truth': 'Old Ground Truth'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output['New Ground Truth'] = gpt_output['Abstract'].apply(get_new_edam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows without mappings\n",
    "gpt_output = gpt_output[~gpt_output['New Ground Truth'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MeSH Terms\n",
    "\n",
    "def get_mesh_terms(abstract):\n",
    "    matching_rows = dataset.loc[dataset['Abstract'] == abstract, 'MeSH Terms']\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        return matching_rows.iloc[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output['MeSH Terms'] = gpt_output['Abstract'].apply(get_mesh_terms)\n",
    "# Drop any rows without mappings\n",
    "gpt_output = gpt_output[~gpt_output['MeSH Terms'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output = gpt_output[['Model', 'Abstract', 'MeSH Terms', 'Old Ground Truth', 'New Ground Truth', 'Predictions', 'Prioritized Predictions']]\n",
    "gpt_output.to_csv(input(\"Enter file name: \"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare old terms with new terms\n",
    "from IPython.display import HTML\n",
    "\n",
    "for idx, row in gpt_output.sample(n=5).iterrows():\n",
    "    display('Abstract:', HTML(f\"<p style='overflow-x: auto'>{row['Abstract']}</p>\"))\n",
    "    # print('Abstract:', row['Abstract'].replace('.', '.\\n'))\n",
    "    print('Old:', row['Old Ground Truth'])\n",
    "    print('New:', ', '.join(row['New Ground Truth']))\n",
    "    print('GPT:', row['Predictions'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PMIDs\n",
    "\n",
    "def get_pmids(abstract):\n",
    "    matching_rows = dataset.loc[dataset['Abstract'] == abstract, 'PMID']\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        return matching_rows.iloc[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids = gpt_output['Abstract'].apply(get_pmids)\n",
    "# Drop any rows without mappings\n",
    "pmids = pmids[~pmids.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids.to_csv('pmids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for discrepancy in MeSH terms\n",
    "\n",
    "There seems to be a disrepancy between the returned xml data and text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez, Medline\n",
    "\n",
    "Entrez.email = \"zqazi@scripps.edu\"\n",
    "\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=21406103, retmode=\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = Entrez.read(handle)\n",
    "handle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_terms = []\n",
    "if \"PubmedArticle\" in article_data:\n",
    "    for article in article_data[\"PubmedArticle\"]:\n",
    "        if \"MeshHeadingList\" in article[\"MedlineCitation\"]:\n",
    "            mesh_headings = article[\"MedlineCitation\"][\"MeshHeadingList\"]\n",
    "            for heading in mesh_headings:\n",
    "                descriptor_name = heading[\"DescriptorName\"]\n",
    "                mesh_terms.append(str(descriptor_name))\n",
    "\n",
    "mesh_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = Entrez.efetch(db=\"pubmed\", id=21406103, rettype='medline', retmode=\"text\")\n",
    "article_data = Medline.parse(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in article_data:\n",
    "    mesh = record.get('MH', '?')\n",
    "    abstract = record.get('AB', '?')\n",
    "\n",
    "print(mesh)\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fixed mesh terms\n",
    "\n",
    "def get_fixed_mesh_terms(pmid):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype='medline', retmode=\"text\")\n",
    "    article_data = Medline.parse(handle)\n",
    "\n",
    "    for record in article_data:\n",
    "        mesh_terms = record.get('MH', None)\n",
    "\n",
    "    return mesh_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dataset['PMID'].apply(get_fixed_mesh_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['MeSH Terms'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data = pd.read_csv('datasets/staging_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data['MeSH Terms'] = output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nde_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
